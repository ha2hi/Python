{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache & Persist\n",
    "Spark에서는 작업 결과를 메모리에 저장할 수 있다. 이때 Cache와 Persist를 사용할 수 있다.  \n",
    "Regression과 같은 반복작업에서 메모리에 저장하여 계속 처리할 수 있기 때문에 효율적이다.  \n",
    "만약 메모리에 데이터를 저장할 충분한 공간이 없다면 디스크를 사용할 수 있다.  \n",
    "\n",
    "- Cache\n",
    "    - RDD : MEMORY_ONLY\n",
    "    - DF : MEMORY_AND_DISK\n",
    "- Persist\n",
    "    - Storage Level을 통해 지정가능\n",
    "- unpersist()\n",
    "    - 캐시 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/12 21:47:20 WARN Utils: Your hostname, MZC01-HYUCKSANGCHO.local resolves to a loopback address: 127.0.0.1; using 192.168.0.80 instead (on interface en0)\n",
      "24/08/12 21:47:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/12 21:47:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"spark-cache\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4865"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCached = spark.read.format(\"parquet\").load(\"./airbnb_listings_parquet\").cache()\n",
    "dfCached.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCached.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan (1)\n",
      "   +- InMemoryRelation (2)\n",
      "         +- * ColumnarToRow (4)\n",
      "            +- Scan parquet  (3)\n",
      "\n",
      "\n",
      "(1) InMemoryTableScan\n",
      "Output [5]: [listing_id#0, listing_url#1, listing_name#2, listing_summary#3, listing_desc#4]\n",
      "Arguments: [listing_id#0, listing_url#1, listing_name#2, listing_summary#3, listing_desc#4]\n",
      "\n",
      "(2) InMemoryRelation\n",
      "Arguments: [listing_id#0, listing_url#1, listing_name#2, listing_summary#3, listing_desc#4], CachedRDDBuilder(org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer@701abd9b,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) ColumnarToRow\n",
      "+- FileScan parquet [listing_id#0,listing_url#1,listing_name#2,listing_summary#3,listing_desc#4] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/mzc01-hyucksangcho/Downloads/airbnb_listings_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<listing_id:int,listing_url:string,listing_name:string,listing_summary:string,listing_desc:...\n",
      ",None)\n",
      "\n",
      "(3) Scan parquet \n",
      "Output [5]: [listing_id#0, listing_url#1, listing_name#2, listing_summary#3, listing_desc#4]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/mzc01-hyucksangcho/Downloads/airbnb_listings_parquet]\n",
      "ReadSchema: struct<listing_id:int,listing_url:string,listing_name:string,listing_summary:string,listing_desc:string>\n",
      "\n",
      "(4) ColumnarToRow [codegen id : 1]\n",
      "Input [5]: [listing_id#0, listing_url#1, listing_name#2, listing_summary#3, listing_desc#4]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# InMemoryTableScan이라는 단계가 추가되었다.\n",
    "dfCached.explain(\"FORMATTED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrame과 Cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4278"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTransformed = dfCached\\\n",
    "    .selectExpr(\"listing_id\", \"listing_name\")\\\n",
    "    .where(col(\"listing_id\") >= 10000000)\\\n",
    "    .cache()\n",
    "    \n",
    "dfTransformed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan (1)\n",
      "   +- InMemoryRelation (2)\n",
      "         +- * Filter (7)\n",
      "            +- InMemoryTableScan (3)\n",
      "                  +- InMemoryRelation (4)\n",
      "                        +- * ColumnarToRow (6)\n",
      "                           +- Scan parquet  (5)\n",
      "\n",
      "\n",
      "(1) InMemoryTableScan\n",
      "Output [1]: [listing_id#0]\n",
      "Arguments: [listing_id#0]\n",
      "\n",
      "(2) InMemoryRelation\n",
      "Arguments: [listing_id#0, listing_name#2], CachedRDDBuilder(org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer@701abd9b,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) Filter (isnotnull(listing_id#0) AND (listing_id#0 >= 10000000))\n",
      "+- InMemoryTableScan [listing_id#0, listing_name#2], [isnotnull(listing_id#0), (listing_id#0 >= 10000000)]\n",
      "      +- InMemoryRelation [listing_id#0, listing_url#1, listing_name#2, listing_summary#3, listing_desc#4], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- *(1) ColumnarToRow\n",
      "               +- FileScan parquet [listing_id#0,listing_url#1,listing_name#2,listing_summary#3,listing_desc#4] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/mzc01-hyucksangcho/Downloads/airbnb_listings_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<listing_id:int,listing_url:string,listing_name:string,listing_summary:string,listing_desc:...\n",
      ",None)\n",
      "\n",
      "(3) InMemoryTableScan\n",
      "Output [2]: [listing_id#0, listing_name#2]\n",
      "Arguments: [listing_id#0, listing_name#2], [isnotnull(listing_id#0), (listing_id#0 >= 10000000)]\n",
      "\n",
      "(4) InMemoryRelation\n",
      "Arguments: [listing_id#0, listing_url#1, listing_name#2, listing_summary#3, listing_desc#4], CachedRDDBuilder(org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer@701abd9b,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) ColumnarToRow\n",
      "+- FileScan parquet [listing_id#0,listing_url#1,listing_name#2,listing_summary#3,listing_desc#4] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/mzc01-hyucksangcho/Downloads/airbnb_listings_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<listing_id:int,listing_url:string,listing_name:string,listing_summary:string,listing_desc:...\n",
      ",None)\n",
      "\n",
      "(5) Scan parquet \n",
      "Output [5]: [listing_id#0, listing_url#1, listing_name#2, listing_summary#3, listing_desc#4]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/mzc01-hyucksangcho/Downloads/airbnb_listings_parquet]\n",
      "ReadSchema: struct<listing_id:int,listing_url:string,listing_name:string,listing_summary:string,listing_desc:string>\n",
      "\n",
      "(6) ColumnarToRow [codegen id : 1]\n",
      "Input [5]: [listing_id#0, listing_url#1, listing_name#2, listing_summary#3, listing_desc#4]\n",
      "\n",
      "(7) Filter [codegen id : 1]\n",
      "Input [2]: [listing_id#0, listing_name#2]\n",
      "Condition : (isnotnull(listing_id#0) AND (listing_id#0 >= 10000000))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cache가 사용되는걸 알 수 있음\n",
    "dfTransformed.select(\"listing_id\").explain(\"FORMATTED\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan (1)\n",
      "   +- InMemoryRelation (2)\n",
      "         +- * Filter (7)\n",
      "            +- InMemoryTableScan (3)\n",
      "                  +- InMemoryRelation (4)\n",
      "                        +- * ColumnarToRow (6)\n",
      "                           +- Scan parquet  (5)\n",
      "\n",
      "\n",
      "(1) InMemoryTableScan\n",
      "Output [1]: [listing_id#0]\n",
      "Arguments: [listing_id#0]\n",
      "\n",
      "(2) InMemoryRelation\n",
      "Arguments: [listing_id#0, listing_name#2], CachedRDDBuilder(org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer@701abd9b,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) Filter (isnotnull(listing_id#0) AND (listing_id#0 >= 30000000))\n",
      "+- InMemoryTableScan [listing_id#0, listing_name#2], [isnotnull(listing_id#0), (listing_id#0 >= 30000000)]\n",
      "      +- InMemoryRelation [listing_id#0, listing_url#1, listing_name#2, listing_summary#3, listing_desc#4], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- *(1) ColumnarToRow\n",
      "               +- FileScan parquet [listing_id#0,listing_url#1,listing_name#2,listing_summary#3,listing_desc#4] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/mzc01-hyucksangcho/Downloads/airbnb_listings_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<listing_id:int,listing_url:string,listing_name:string,listing_summary:string,listing_desc:...\n",
      ",None)\n",
      "\n",
      "(3) InMemoryTableScan\n",
      "Output [2]: [listing_id#0, listing_name#2]\n",
      "Arguments: [listing_id#0, listing_name#2], [isnotnull(listing_id#0), (listing_id#0 >= 30000000)]\n",
      "\n",
      "(4) InMemoryRelation\n",
      "Arguments: [listing_id#0, listing_url#1, listing_name#2, listing_summary#3, listing_desc#4], CachedRDDBuilder(org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer@701abd9b,StorageLevel(disk, memory, deserialized, 1 replicas),*(1) ColumnarToRow\n",
      "+- FileScan parquet [listing_id#0,listing_url#1,listing_name#2,listing_summary#3,listing_desc#4] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/mzc01-hyucksangcho/Downloads/airbnb_listings_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<listing_id:int,listing_url:string,listing_name:string,listing_summary:string,listing_desc:...\n",
      ",None)\n",
      "\n",
      "(5) Scan parquet \n",
      "Output [5]: [listing_id#0, listing_url#1, listing_name#2, listing_summary#3, listing_desc#4]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/Users/mzc01-hyucksangcho/Downloads/airbnb_listings_parquet]\n",
      "ReadSchema: struct<listing_id:int,listing_url:string,listing_name:string,listing_summary:string,listing_desc:string>\n",
      "\n",
      "(6) ColumnarToRow [codegen id : 1]\n",
      "Input [5]: [listing_id#0, listing_url#1, listing_name#2, listing_summary#3, listing_desc#4]\n",
      "\n",
      "(7) Filter [codegen id : 1]\n",
      "Input [2]: [listing_id#0, listing_name#2]\n",
      "Condition : (isnotnull(listing_id#0) AND (listing_id#0 >= 30000000))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 그렇다면 변수로 저장하지 않고 Cache를하면 어떻게 될까?\n",
    "# 새로운 DataFrame으로 저장하지 않아도 캐싱이 되는걸 알 수 있다.\n",
    "dfCached.selectExpr(\"listing_id\", \"listing_name\").where(col(\"listing_id\") >= 30000000).cache()\n",
    "dfCached.selectExpr(\"listing_id\", \"listing_name\").where(col(\"listing_id\") >= 30000000).select(\"listing_id\").explain(\"FORMATTED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL과 Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|listing_id|         listing_url|        listing_name|     listing_summary|        listing_desc|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  12276698|https://www.airbn...|Downtown Casa in ...|Built in (Phone n...|Built in (Phone n...|\n",
      "|  39589825|https://www.airbn...|Comfy Stapleton c...|                null|                null|\n",
      "|  16676955|https://www.airbn...|Adorable Row Home...|Updated Spanish s...|Updated Spanish s...|\n",
      "|  38638676|https://www.airbn...|Amenity Rich LUX ...|                null|                null|\n",
      "|  33396764|https://www.airbn...|NE Dnvr Home 3br/...|                null|                null|\n",
      "|   9842499|https://www.airbn...|Lg Light Bsmnt in...|Dbl BR and den w/...|Dbl BR and den w/...|\n",
      "|  39390474|https://www.airbn...|Sonder | Universi...|Featured in The N...|Featured in The N...|\n",
      "|  18503556|https://www.airbn...|Spacious Studio i...|FREE WIFI on PH a...|FREE WIFI on PH a...|\n",
      "|  40239550|https://www.airbn...|Charming Private ...|Hi! Welcome. We l...|Hi! Welcome. We l...|\n",
      "|  39294702|https://www.airbn...|Private Apartment...|This spacious, we...|This spacious, we...|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCached.createOrReplaceTempView(\"RAW\")\n",
    "spark.sql(\"SELECT * FROM RAW LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CACHE TABLE을 통해 캐싱한다.\n",
    "spark.sql(\"\"\"\n",
    "CACHE TABLE RAW_CACHED AS SELECT * FROM RAW\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNCACHE TABLE을 통해 캐싱을 제거한다.\n",
    "spark.sql(\"\"\"\n",
    "UNCACHE TABLE IF EXISTS RAW_CACHED\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
