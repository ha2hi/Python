{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스파크 실행구조\n",
    "1. 사용자는 spark-submit을 사용하여 애플리케이션을 제출한다.\n",
    "2. spark-submit은 스파크 드라이버를 실행하고 사용자가 정의한 main() 메소드를 호출한다.\n",
    "3. 드라이버 프로그램은 클러스터 매니저에게 익스큐터 실행을 위한 리소스를 요청한다.\n",
    "4. 클러스터 매니저는 드라이버 프로그램을 대신해 익스큐터를 실행한다.\n",
    "5. 드라이버는 작업 내역을 단위 작업 형태로 나눠 익스큐터에게 보낸다.\n",
    "6. 익스큐터에서 작업이 실행된다.\n",
    "7. 작업이 끝나면 익스큐터들은 중지되고 클러스터 매니저에 사용했던 자원을 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark-submit\n",
    "모든 클러스터 매니저 간에 작업을 제출하기 위해서 spark-submit을 사용할 수 있습니다.  \n",
    "- 로컬에서 스파크 실행시\n",
    "```\n",
    "bin/spark-submit my_script.py\n",
    "```\n",
    "- 스파크 단독 클러스터 매니저에게 제출 시\n",
    "```\n",
    "bin/spark-submit --master spark://host:7077 --executor-memory 10g my_script.py\n",
    "```\n",
    "- --master 플래그\n",
    "  - spark://host:port\n",
    "    - 스파크 단독 클러스터의 지정한 포트로 접속한다. 기본적으로 7077 포트 사용\n",
    "  - mesos://host:port\n",
    "    - 메소스 클러스터에 지정한 포트로 접속한다 기본적으로 5060 포트 사용\n",
    "  - yarn\n",
    "    - 얀 클러스터에 접속한다. YARN에서 실행할 때는 하둡 설정 디렉터리를 HADOOP_CONF_DIR 환경 변수에 설정해야됨\n",
    "  - local\n",
    "    - 로컬 모드에서 싱글 코어로 실행한다.\n",
    "  - local[N]\n",
    "    - 로컬 모드에서 N개 코어로 실행한다.\n",
    "  - local[*]\n",
    "    - 로컬 모드에서 머신이 갖고 있는 만큼의 코어로 실행한다.\n",
    "- spark-submit 플러그\n",
    "  - --master\n",
    "    - 접속할 클러스터 매니저를 가르킴\n",
    "  - --deploy-mode\n",
    "    - 드라이버 프로그램이 지역적으로 실행될지(\"client\") 클러스터의 작업 머신들 중 실행될지(\"cluster\") 결정한다.\n",
    "  - --name\n",
    "    - 스파크 웹 UI에 표시될 이름\n",
    "  - --jars\n",
    "    - 사용자 애플리케이션의 클래스패스에 있어야할 jar 파일 목록\n",
    "  - --files\n",
    "    - 애플리케이션의 작업 디렉터리 내에 필요한 파일들의 목록\n",
    "  - --py-files\n",
    "    - 애플리케이션의 PYTHON-PATH에 추가되어야할 파일 목록\n",
    "  - --executor-memory\n",
    "    - 익스큐터가 쓸 메모리를 바이트 단위로 지정\n",
    "  - --driver-memory\n",
    "    - 드라이버 프로세스가 쓸 메모리를 바이트 단위로 지정\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용자 코드와 의존성 라이브러리 패키징 하기\n",
    "pyspark로 외부라이브러리를 사용 방법\n",
    "1. 직접 설치\n",
    "- 클러스터 머신들에 기본 파이썬 패키지 매니저를 써서(pip) 직접 설치 및 site-package/ 밑에 파일을 전달하는 방법이 있다.\n",
    "2. spark-submit 인자\n",
    "- --py-files를 써서 개별 라이브러리를 제출 하여 인터프리터 경로에 추가할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
